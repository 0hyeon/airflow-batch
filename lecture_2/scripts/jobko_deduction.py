from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import col, unix_timestamp, lit, trim, udf, lag
from pyspark.sql.types import StringType
import traceback

spark = SparkSession.builder.appName("deduction_processing_final_exact").getOrCreate()

# ë‚ ì§œ ì„¤ì •
yesterday = "2025-04-15"
S3_BUCKET = "fc-practice2"
S3_INPUT_PREFIX = f"apps_flyer_jobko/date={yesterday}/"
S3_OUTPUT_PREFIX = f"apps_flyer_jobko/deduction_results/{yesterday}/"
output_s3_path = f"s3a://{S3_BUCKET}/{S3_OUTPUT_PREFIX}"

files = [
    "data_aos_onepick_retarget.parquet", "data_aos_onepick_ua.parquet",
    "data_aos_retarget.parquet", "data_aos_ua.parquet",
    "data_ios_onepick_retarget.parquet", "data_ios_onepick_ua.parquet",
    "data_ios_retarget.parquet", "data_ios_ua.parquet"
]

columns_to_keep = [
    "Attributed_Touch_Type","Attributed_Touch_Time","Install_Time","Event_Time","Event_Name","Partner",
    "Media_Source","Channel","Keywords","Campaign","Adset","Ad","Ad_Type","Region","Country_Code",
    "Carrier","Language","AppsFlyer_ID","Android_ID","Advertising_ID","IDFA","IDFV","Device_Category",
    "Platform","OS_Version","App_Version","SDK_Version","Is_Retargeting","Retargeting_Conversion_Type",
    "Is_Primary_Attribution","Attribution_Lookback","Reengagement_Window","Match_Type","User_Agent",
    "Conversion_Type","Campaign_Type","Device_Model","Keyword_ID","Original_URL"
]

event_transfer = {
    "1st_update_complete": "7 ì´ë ¥ì„œê¸°ë³¸ì—…ë°ì´íŠ¸", "business_signup_complete": "4 ê¸°ì—…íšŒì›ê°€ì…",
    "job_apply_complete_homepage": "3 í™ˆí˜ì´ì§€ì§€ì›", "job_apply_complete_mobile": "2 ì¦‰ì‹œì§€ì›",
    "minikit_complete": "9 MICT", "re_update_complete": "8 ì´ë ¥ì„œì¶”ê°€ì—…ë°ì´íŠ¸",
    "resume_complete_1st": "5 ì´ë ¥ì„œìµœì´ˆë“±ë¡", "resume_complete_re": "6 ì´ë ¥ì„œì¶”ê°€ë“±ë¡",
    "signup_all": "1 íšŒì›ê°€ì…", "jobposting_complete": "90 ì±„ìš©ê³µê³ ë“±ë¡",
    "1pick_direct_apply": "93 ì›í”½ì§ì ‘ì§€ì›", "1pick_offer_apply": "92 ì›í”½ì œì•ˆì§€ì›",
    "1pick_view_jobposting": "94 ì›í”½ê³µê³ ë·°", "nhis_resistration_complete": "91 ê±´ê°•ë³´í—˜_ë“±ë¡ì™„ë£Œ",
    "1pick_start": "95 ì›í”½í•˜ê¸°", "event_2411_tvc_apply": "96 TVCì‘ëª¨ì™„ë£Œ",
    "high_view_jobposting": "í•˜ì´í…Œí¬_ê³µê³ ì¡°íšŒ", "high_scrap_notice": "í•˜ì´í…Œí¬_ê³µê³ ìŠ¤í¬ë©",
    "high_direct_apply": "í•˜ì´í…Œí¬_ì¦‰ì‹œì§€ì›", "high_homepage_apply": "í•˜ì´í…Œí¬_í™ˆí˜ì´ì§€ì§€ì›",
    "careercheck_assess_complete": "ì»¤ì²µ_ì°¸ê²¬í•˜ê¸°",
}
map_event_udf = udf(lambda x: event_transfer.get(x, x), StringType())

def normalize_column_names(df):
    new_columns = [col_name.strip().replace(" ", "_").replace("-", "_") for col_name in df.columns]
    return df.toDF(*new_columns)

AOS_MEDIA_SOURCE = ["appier_int", "adisonofferwall_int", "cashfriends_int", "greenp_int", "buzzad_int"]
IOS_MEDIA_SOURCE = ["adisonofferwall_int", "cashfriends_int", "greenp_int", "buzzad_int"]

combined_rows = []

for file in files:
    try:
        print(f"ğŸš€ ì²˜ë¦¬ì¤‘: {file}")
        df = spark.read.parquet(f"s3a://{S3_BUCKET}/{S3_INPUT_PREFIX}{file}")
        df = normalize_column_names(df)
        for colname in df.columns:
            df = df.withColumn(colname, trim(col(colname).cast("string")))
        df = df.select(*[c for c in df.columns if c in columns_to_keep])

        lower_name = file.lower()
        platform = "aos" if "aos" in lower_name else "ios"
        id_col = "Advertising_ID" if platform == "aos" else "IDFA"
        media_sources = AOS_MEDIA_SOURCE if platform == "aos" else IOS_MEDIA_SOURCE

        # === 1. Retarget â†’ í”„ë¡œë“œ ===
        if "retarget" in lower_name:
            df_re = df.filter(col("Retargeting_Conversion_Type") == "re-engagement")\
                .withColumn("event_ts", unix_timestamp("Event_Time", "yyyy-MM-dd HH:mm:ss.SSS"))\
                .withColumn("prev2", lag("event_ts", 2).over(Window.partitionBy(id_col).orderBy("event_ts")))\
                .withColumn("delta2", col("event_ts") - col("prev2"))
            df_prod = df_re.filter((col("delta2") <= 60) & col("Media_Source").isin(media_sources))\
                .withColumn("Event_Name", map_event_udf("Event_Name"))\
                .withColumn("êµ¬ë¶„", lit("í”„ë¡œë“œ"))\
                .select(*columns_to_keep, "êµ¬ë¶„")
            combined_rows.append(df_prod)
            df = df.join(df_prod.select("Event_Time", id_col), on=["Event_Time", id_col], how="left_anti")

        # === 2. ITET ===
        df = df.withColumn("install_ts", unix_timestamp("Install_Time", "yyyy-MM-dd HH:mm:ss.SSS"))\
               .withColumn("event_ts", unix_timestamp("Event_Time", "yyyy-MM-dd HH:mm:ss.SSS"))\
               .withColumn("itet_diff", (col("event_ts") - col("install_ts")) / 3600.0)

        df_itet = df.filter(col("itet_diff") >= 24 * 1.05)\
            .withColumn("Event_Name", map_event_udf("Event_Name"))\
            .withColumn("êµ¬ë¶„", lit("ITET"))\
            .select(*columns_to_keep, "êµ¬ë¶„")
        combined_rows.append(df_itet)
        df = df.filter(col("itet_diff") < 24 * 1.05)

        # === 3. CTIT ===
        if "Attributed_Touch_Time" in df.columns:
            df = df.withColumn("touch_ts", unix_timestamp("Attributed_Touch_Time", "yyyy-MM-dd HH:mm:ss.SSS"))\
                   .withColumn("ctit_diff", (col("install_ts") - col("touch_ts")) / 3600.0)
            df_ctit = df.filter(col("ctit_diff") >= 24 * 1.05)\
                .withColumn("Event_Name", map_event_udf("Event_Name"))\
                .withColumn("êµ¬ë¶„", lit("CTIT"))\
                .select(*columns_to_keep, "êµ¬ë¶„")
            combined_rows.append(df_ctit)
            df = df.filter(col("ctit_diff") < 24 * 1.05)

        # === 4. FALSE ===
        if "ua" in lower_name:
            df_false = df.filter(col("Is_Primary_Attribution") == "false")\
                .withColumn("Event_Name", map_event_udf("Event_Name"))\
                .withColumn("êµ¬ë¶„", lit("FALSE"))\
                .select(*columns_to_keep, "êµ¬ë¶„")
            combined_rows.append(df_false)
            df = df.filter(col("Is_Primary_Attribution") != "false")

        # === 5. ì •ìƒ / ì›í”½ ===
        df_normal = df.withColumn("Event_Name", map_event_udf("Event_Name"))\
                      .withColumn("êµ¬ë¶„", lit("ì •ìƒ"))\
                      .select(*columns_to_keep, "êµ¬ë¶„")
        combined_rows.append(df_normal)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ - {file}: {e}")
        traceback.print_exc()

# ë³‘í•© ë° ì €ì¥
if combined_rows:
    result_df = combined_rows[0]
    for df in combined_rows[1:]:
        result_df = result_df.unionByName(df)
    result_df.coalesce(1).write.option("header", True).mode("overwrite").csv(output_s3_path)
    print("âœ… ì €ì¥ ì™„ë£Œ:", output_s3_path)
else:
    print("âš ï¸ ì €ì¥í•  ë°ì´í„° ì—†ìŒ")

spark.stop()
