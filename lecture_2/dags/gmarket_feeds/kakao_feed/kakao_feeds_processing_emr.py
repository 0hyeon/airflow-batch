from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.amazon.aws.sensors.emr import EmrJobFlowSensor, EmrStepSensor
from airflow.providers.amazon.aws.operators.emr import EmrTerminateJobFlowOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime, timedelta
import boto3

# --- ì‚¬ìš©ì ì„¤ì • ë³€ìˆ˜ ---#
S3_BUCKET = "gyoung0-test"
AWS_CONN_ID = "aws_conn_id"
EMR_EC2_KEY_NAME = "test"

# S3 ê²½ë¡œ ì •ì˜
# Gë§ˆì¼“/ì˜¥ì…˜ í”¼ë“œ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥ìœ¼ë¡œ ì§€ì •
GMARKET_FEED_KEY = "feeds/general/combined_gmarket_feed.csv.gz"
AUCTION_FEED_KEY = "feeds/general/combined_auction_feed.csv.gz"

# ERì—ì„œ ì‹¤í–‰í•  ìƒˆë¡œìš´ Spark ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
SCRIPT_S3_KEY = "scripts/kakao_process_gmarket_feeds.py"
# EMR ì²˜ë¦¬ ê²°ê³¼ê°€ ì €ì¥ë  ê²½ë¡œ
OUTPUT_S3_PATH = f"s3://{S3_BUCKET}/processed-feeds/gmarket_auction/"
LOG_S3_PATH = f"s3://{S3_BUCKET}/emr-logs/"

# --- DAG ê¸°ë³¸ ì„¤ì • ---
default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 7, 30),
    "aws_conn_id": AWS_CONN_ID,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

# --- DAG ì •ì˜ ---
# DAG IDì™€ íƒœê·¸ë¥¼ ìƒˆë¡œìš´ ëª©ì ì— ë§ê²Œ ìˆ˜ì •
with DAG(
    dag_id="kakao_gmarket_auction_emr_processing",
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
    tags=["emr", "gmarket", "auction", "kakao"],
    doc_md="Gë§ˆì¼“ê³¼ ì˜¥ì…˜ í”¼ë“œ íŒŒì¼ì„ EMR Sparkë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
) as dag:

    # --- Python í•¨ìˆ˜ ì •ì˜ ---
    def create_emr_cluster(**kwargs):
        """Boto3ë¥¼ ì‚¬ìš©í•˜ì—¬ EMR í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
        s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        credentials = s3_hook.get_credentials()
        client = boto3.client(
            "emr",
            aws_access_key_id=credentials.access_key,
            aws_secret_access_key=credentials.secret_key,
            region_name=s3_hook.conn_region_name,
        )

        # [ë³€ê²½] EMR í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìˆ˜ì •
        response = client.run_job_flow(
            Name="gmarket-auction-feed-processing-cluster",
            ReleaseLabel="emr-6.15.0",
            Applications=[{"Name": "Spark"}],
            Instances={
                "InstanceGroups": [
                    {
                        "Name": "Master node",
                        "Market": "ON_DEMAND",
                        "InstanceRole": "MASTER",
                        "InstanceType": "m5.xlarge",
                        "InstanceCount": 1,
                    },
                    {
                        "Name": "Worker nodes",
                        "Market": "ON_DEMAND",
                        "InstanceRole": "CORE",
                        "InstanceType": "m5.xlarge",
                        "InstanceCount": 2,
                    },
                ],
                "Ec2KeyName": EMR_EC2_KEY_NAME,
                "KeepJobFlowAliveWhenNoSteps": True,
            },
            JobFlowRole="EMR_EC2_DefaultRole",
            ServiceRole="EMR_DefaultRole",
            LogUri=LOG_S3_PATH,
            AutoTerminationPolicy={"IdleTimeout": 600},
            VisibleToAllUsers=True,
        )
        kwargs["ti"].xcom_push(key="emr_cluster_id", value=response["JobFlowId"])
        print(f"âœ… Created EMR cluster: {response['JobFlowId']}")

    def submit_spark_job(**kwargs):
        """EMR í´ëŸ¬ìŠ¤í„°ì— Spark ì‘ì—…ì„ ì œì¶œí•˜ëŠ” í•¨ìˆ˜"""
        ti = kwargs["ti"]
        cluster_id = ti.xcom_pull(
            task_ids="create_emr_cluster_task", key="emr_cluster_id"
        )
        s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        credentials = s3_hook.get_credentials()
        client = boto3.client(
            "emr",
            aws_access_key_id=credentials.access_key,
            aws_secret_access_key=credentials.secret_key,
            region_name=s3_hook.conn_region_name,
        )

        # [ë³€ê²½] Spark ìŠ¤í¬ë¦½íŠ¸ì— ì…ë ¥ ë° ì¶œë ¥ íŒŒì¼ ê²½ë¡œë¥¼ ì¸ìë¡œ ì „ë‹¬
        spark_submit_args = [
            "spark-submit",
            "--deploy-mode",
            "cluster",
            f"s3://{S3_BUCKET}/{SCRIPT_S3_KEY}",
            "--gmarket-input",
            f"s3://{S3_BUCKET}/{GMARKET_FEED_KEY}",
            "--auction-input",
            f"s3://{S3_BUCKET}/{AUCTION_FEED_KEY}",
            "--output",
            OUTPUT_S3_PATH,
        ]

        # [ë³€ê²½] EMR Step ì´ë¦„ ìˆ˜ì •
        response = client.add_job_flow_steps(
            JobFlowId=cluster_id,
            Steps=[
                {
                    "Name": "Process_Gmarket_Auction_Feeds",
                    "ActionOnFailure": "CONTINUE",
                    "HadoopJarStep": {
                        "Jar": "command-runner.jar",
                        "Args": spark_submit_args,
                    },
                }
            ],
        )
        ti.xcom_push(key="spark_step_id", value=response["StepIds"][0])
        print(f"âœ… Submitted Spark job with step ID: {response['StepIds'][0]}")

    # --- ğŸ“ Airflow íƒœìŠ¤í¬ ì •ì˜ ---

    # 1. [ë³€ê²½] Gë§ˆì¼“ í”¼ë“œ íŒŒì¼ ëŒ€ê¸° ì„¼ì„œ
    wait_for_gmarket_feed_task = S3KeySensor(
        task_id="wait_for_gmarket_feed_task",
        bucket_name=S3_BUCKET,
        bucket_key=GMARKET_FEED_KEY,
        poke_interval=60,
        timeout=60 * 10,
    )

    # 2. [ì¶”ê°€] ì˜¥ì…˜ í”¼ë“œ íŒŒì¼ ëŒ€ê¸° ì„¼ì„œ
    wait_for_auction_feed_task = S3KeySensor(
        task_id="wait_for_auction_feed_task",
        bucket_name=S3_BUCKET,
        bucket_key=AUCTION_FEED_KEY,
        poke_interval=60,
        timeout=60 * 10,
    )

    # 3. EMR í´ëŸ¬ìŠ¤í„° ìƒì„± íƒœìŠ¤í¬
    create_emr_cluster_task = PythonOperator(
        task_id="create_emr_cluster_task",
        python_callable=create_emr_cluster,
    )

    # 4. EMR í´ëŸ¬ìŠ¤í„° ì¤€ë¹„ ì™„ë£Œ ëŒ€ê¸°
    wait_for_emr_cluster_task = EmrJobFlowSensor(
        task_id="wait_for_emr_cluster_task",
        job_flow_id="{{ ti.xcom_pull(task_ids='create_emr_cluster_task', key='emr_cluster_id') }}",
        target_states=["WAITING"],
    )

    # 5. Spark ì‘ì—… ì œì¶œ
    submit_spark_job_task = PythonOperator(
        task_id="submit_spark_job_task",
        python_callable=submit_spark_job,
    )

    # 6. Spark ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
    wait_for_spark_step_task = EmrStepSensor(
        task_id="wait_for_spark_step_task",
        job_flow_id="{{ ti.xcom_pull(task_ids='create_emr_cluster_task', key='emr_cluster_id') }}",
        step_id="{{ ti.xcom_pull(task_ids='submit_spark_job_task', key='spark_step_id') }}",
        target_states=["COMPLETED"],
    )

    # 7. EMR í´ëŸ¬ìŠ¤í„° ì¢…ë£Œ
    terminate_emr_cluster_task = EmrTerminateJobFlowOperator(
        task_id="terminate_emr_cluster_task",
        job_flow_id="{{ ti.xcom_pull(task_ids='create_emr_cluster_task', key='emr_cluster_id') }}",
        trigger_rule="all_done",
    )

    # --- ğŸš€ DAG ì‹¤í–‰ ìˆœì„œ ì •ì˜ ---
    # ë‘ í”¼ë“œ íŒŒì¼ì´ ëª¨ë‘ ì¤€ë¹„ë˜ë©´ EMR í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œì‘
    [wait_for_gmarket_feed_task, wait_for_auction_feed_task] >> create_emr_cluster_task

    create_emr_cluster_task >> wait_for_emr_cluster_task >> submit_spark_job_task
    submit_spark_job_task >> wait_for_spark_step_task >> terminate_emr_cluster_task
