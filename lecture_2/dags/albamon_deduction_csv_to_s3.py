from airflow import DAG
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime, timedelta
import os
# from plugins import slack 
#
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 5, 13),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'on_failure_callback': slack.on_failure_callback,  # ðŸš¨ðŸš¨ðŸ“¢Slack ì•Œë¦¼ ì¶”ê°€
}

dag = DAG(
    'albamon_process_and_upload_deduction_to_s3_with_hook',
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
)


def upload_to_s3_with_hook(**context):
    s3_hook = S3Hook(aws_conn_id='aws_conn_id')  # ì—°ê²° IDì— ë§žê²Œ ì„¤ì •
    bucket = 'gyoung0-test'
    target_date = context["dag_run"].conf.get("target_date")
    print(f"ðŸ“¦ ì—…ë¡œë“œ ëŒ€ìƒ ë‚ ì§œ: {target_date}")
    s3_key = f'deduction-csv/date={target_date}/final_attachment_albamon.csv'
    local_file_path = '/dags/data/final_attachment_albamon.csv'

    if not os.path.exists(local_file_path):
        raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŒ: {local_file_path}")

    s3_hook.load_file(
        filename=local_file_path,
        key=s3_key,
        bucket_name=bucket,
        replace=True
    )
    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: s3://{bucket}/{s3_key}")

upload_task = PythonOperator(
    task_id='upload_deduction_csv_to_s3',
    python_callable=upload_to_s3_with_hook,
    provide_context=True,  # ì´ê±° ë¹ ì§€ë©´ context ëª» ì”€
    dag=dag,
)

trigger_processing = TriggerDagRunOperator(
    task_id='trigger_process_dag',
    trigger_dag_id='albamon_iceberg_to_s3_emr',  # ì´ DAG IDë¡œ í˜¸ì¶œ
    conf={
        "target_date": "{{ dag_run.conf['target_date'] }}"
    },
    dag=dag,
)

upload_task >> trigger_processing