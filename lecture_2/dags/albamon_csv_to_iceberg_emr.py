from airflow import DAG
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime, timedelta
import time
from airflow.models import Variable
from airflow.sensors.python import PythonSensor
# from plugins import slack 
import botocore.exceptions


# â—ï¸ë²„í‚· ê²½ë¡œ ìˆ˜ì •
S3_BUCKET = "gyoung0-test"
# â—ï¸connections s3 ì´ë¦„ ì„¤ì • 
AWS_CONN_ID = "aws_conn_id"

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2025, 6, 2),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    # 'on_failure_callback': slack.on_failure_callback,  # ðŸš¨ðŸš¨ðŸ“¢Slack ì•Œë¦¼ ì¶”ê°€
}

dag = DAG(
    "albamon_iceberg_to_s3_emr",
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
)

# âœ… 1. EMR í´ëŸ¬ìŠ¤í„° ìƒì„±
def create_emr_cluster(**kwargs):
    import boto3
    session = boto3.Session(
        aws_access_key_id=Variable.get("AWS_ACCESS_KEY"),
        aws_secret_access_key=Variable.get("AWS_SECRET_KEY"),
        region_name=Variable.get("AWS_DEFAULT_REGION"),
    )
    client = session.client("emr")

    response = client.run_job_flow(
        Name="csv-to-iceberg-emr-cluster-albamon",
        ReleaseLabel="emr-6.15.0",
        Applications=[{"Name": "Spark"}],
        Instances={
            "InstanceGroups": [
                {
                    "Name": "Master node",
                    "Market": "ON_DEMAND",
                    "InstanceRole": "MASTER",
                    "InstanceType": "m5.xlarge",
                    "InstanceCount": 1,
                },
                {
                    "Name": "Worker nodes",
                    "Market": "ON_DEMAND",
                    "InstanceRole": "CORE",
                    "InstanceType": "m5.xlarge",
                    "InstanceCount": 2,
                },
            ],
            "Ec2KeyName": "test",
            "KeepJobFlowAliveWhenNoSteps": True,  # ëª¨ë“  Stepì´ ì™„ë£Œë˜ëŠ” ì¦‰ì‹œ EMR í´ëŸ¬ìŠ¤í„°ê°€ ì¦‰ì‹œ ìžë™
            "TerminationProtected": False,
        },
        JobFlowRole="EMR_EC2_DefaultRole",
        ServiceRole="EMR_DefaultRole",
        LogUri=f"s3://{S3_BUCKET}/emr-logs/",
        AutoTerminationPolicy={"IdleTimeout": 600}, # âœ… 10ë¶„ ëŒ€ê¸° í›„ ìžë™ ì¢…ë£Œ
        VisibleToAllUsers=True,
    )

    cluster_id = response["JobFlowId"]
    kwargs["ti"].xcom_push(key="emr_cluster_id", value=cluster_id)
    print(f"âœ… EMR í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ: {cluster_id}")

create_emr = PythonOperator(
    task_id="create_emr",
    python_callable=create_emr_cluster,
    # provide_context=True,
    dag=dag,
)


# âœ… 2. í´ëŸ¬ìŠ¤í„°ê°€ `WAITING` ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°
def wait_for_emr_cluster(**kwargs):
    import boto3
    session = boto3.Session(
        aws_access_key_id=Variable.get("AWS_ACCESS_KEY"),
        aws_secret_access_key=Variable.get("AWS_SECRET_KEY"),
        region_name=Variable.get("AWS_DEFAULT_REGION"),
    )
    client = session.client("emr")
    cluster_id = kwargs["ti"].xcom_pull(task_ids="create_emr", key="emr_cluster_id")

    response = client.describe_cluster(ClusterId=cluster_id)
    state = response["Cluster"]["Status"]["State"]
    print(f"âŒ› í˜„ìž¬ í´ëŸ¬ìŠ¤í„° ìƒíƒœ: {state}")

    if state == "WAITING":
        return True
    elif state in ["TERMINATING", "TERMINATED", "TERMINATED_WITH_ERRORS"]:
        raise Exception(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id} ë¹„ì •ìƒ ì¢…ë£Œë¨! ìƒíƒœ: {state}")
    else:
        return False


wait_for_cluster = PythonSensor(
    task_id="wait_for_cluster",
    python_callable=wait_for_emr_cluster,
    # provide_context=True,
    mode='reschedule',
    poke_interval=60,          # 1ë¶„ë§ˆë‹¤ ì²´í¬
    timeout=60 * 60 * 2,       # ìµœëŒ€ 1ì‹œê°„ ê¸°ë‹¤ë¦¼
    dag=dag,
)

# âœ… 3. S3 íŒŒì¼ ì¡´ìž¬ ì—¬ë¶€ í™•ì¸ 
def check_s3_file_with_hook(**kwargs):
    hook = S3Hook(aws_conn_id=AWS_CONN_ID)
    target_date = kwargs["dag_run"].conf.get("target_date")
    key = f"deduction-csv/date={target_date}/final_attachment_albamon.csv"

    if not hook.check_for_key(key, bucket_name=S3_BUCKET):
        raise FileNotFoundError(f"âŒ S3ì— íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŒ: s3://{S3_BUCKET}/{key}")

    print(f"âœ… S3ì— íŒŒì¼ ì¡´ìž¬ í™•ì¸ ì™„ë£Œ: s3://{S3_BUCKET}/{key}")

check_s3_files = PythonOperator(
    task_id="check_s3_files",
    python_callable=check_s3_file_with_hook,
    # provide_context=True,
    dag=dag,
)
# âœ… 4. EMRì—ì„œ PySpark ìž‘ì—… ì‹¤í–‰
def submit_spark_job(**kwargs):
    import boto3
    session = boto3.Session(
        aws_access_key_id=Variable.get("AWS_ACCESS_KEY"),
        aws_secret_access_key=Variable.get("AWS_SECRET_KEY"),
        region_name=Variable.get("AWS_DEFAULT_REGION"),
    )
    target_date = kwargs["dag_run"].conf.get("target_date")
    print(f" target_date: {target_date}")
    client = session.client("emr")
    cluster_id = kwargs["ti"].xcom_pull(task_ids="create_emr", key="emr_cluster_id")

    response = client.add_job_flow_steps(
        JobFlowId=cluster_id,
        Steps=[
            {
                "Name": "Process Parquet with PySpark",
                "ActionOnFailure": "CONTINUE",
                "HadoopJarStep": {
                    "Jar": "command-runner.jar",
                    "Args": [
                        "spark-submit",
                        "--deploy-mode", "cluster",
                        "--conf", "spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
                        "--conf", "spark.sql.catalog.glue=org.apache.iceberg.spark.SparkCatalog",
                        "--conf", "spark.sql.catalog.glue.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog",
                        "--conf", "spark.sql.catalog.glue.warehouse=s3a://gyoung0-test/iceberg_warehouse_albamon_appsflyer/",
                        "--conf", "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem",
                        "--conf", f"spark.hadoop.yesterday={target_date}",
                        "--conf", "spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain",
                        "--packages", "org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2",
                        "s3://gyoung0-test/scripts/csv_to_iceberg_albamon.py"  # âœ… PySpark ì½”ë“œ ìœ„ì¹˜
                    ],
                },
            }
        ],
    )

    step_id = response["StepIds"][0]
    kwargs["ti"].xcom_push(key="spark_step_id", value=step_id)
    print(f"âœ… PySpark ìž‘ì—… ì œì¶œ ì™„ë£Œ: Step ID = {step_id}")

run_spark_job = PythonOperator(
    task_id="run_spark_job",
    python_callable=submit_spark_job,
    dag=dag,
)

# âœ… 5. ìž‘ì—… ì™„ë£Œ í›„ í´ëŸ¬ìŠ¤í„° ìžë™ ì¢…ë£Œ
def wait_for_spark_job(**kwargs):
    import boto3
    session = boto3.Session(
        aws_access_key_id=Variable.get("AWS_ACCESS_KEY"),
        aws_secret_access_key=Variable.get("AWS_SECRET_KEY"),
        region_name=Variable.get("AWS_DEFAULT_REGION"),
    )
    client = session.client("emr")
    cluster_id = kwargs["ti"].xcom_pull(task_ids="create_emr", key="emr_cluster_id")
    step_id = kwargs["ti"].xcom_pull(task_ids="run_spark_job", key="spark_step_id")

    response = client.describe_step(ClusterId=cluster_id, StepId=step_id)
    state = response["Step"]["Status"]["State"]
    print(f"âŒ› í˜„ìž¬ Spark Step ìƒíƒœ: {state}")

    if state == "COMPLETED":
        client.terminate_job_flows(JobFlowIds=[cluster_id])
        print(f"ðŸ›‘ í´ëŸ¬ìŠ¤í„° {cluster_id} ì¢…ë£Œ ìš”ì²­ ì™„ë£Œ")
        return True
    elif state in ["FAILED", "CANCELLED"]:
        raise Exception(f"âŒ Spark ìž‘ì—… ì‹¤íŒ¨ ìƒíƒœ: {state}")
    else:
        return False

wait_for_spark = PythonSensor(
    task_id="wait_for_spark",
    python_callable=wait_for_spark_job,
    mode='reschedule',
    poke_interval=60,          # 1ë¶„ë§ˆë‹¤ ì²´í¬
    timeout=60 * 60 * 1,       # ìµœëŒ€ 1ì‹œê°„ ê¸°ë‹¤ë¦¼

    dag=dag,
)

check_s3_files >> create_emr >> wait_for_cluster >> run_spark_job >> wait_for_spark
